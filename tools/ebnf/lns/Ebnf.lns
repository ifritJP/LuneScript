// -*- coding: utf-8 -*-

import go/github:com.ifritJP.LuneScript.src.lune.base.Tokenizer as LnsTokenizer;
import go/github:com.ifritJP.LuneScript.src.lune.base.Types as LnsTypes;


pub class EbnfTokenizer {
   let tokenizer:LnsTokenizer.PushbackTokenizer;
   let mut inEbnf:bool {pub,pub};

   fn __init( tokenizer:LnsTokenizer.Tokenizer ) {
      self.tokenizer = new LnsTokenizer.DefaultPushbackTokenizer( tokenizer );
      self.inEbnf = false;
   }

   pub static fn create() : EbnfTokenizer {
      let path = "../../../ifritJP.github.io/hugo/content/LuneScript/ebnf.ja.org";
      let mut tokenizer = LnsTokenizer.StreamTokenizer.create(
         LnsTypes.TokenizerSrc.LnsPath( nil, path, "ebnf", nil ), false, nil, nil );
      return new EbnfTokenizer( tokenizer );
   }

   pri fn checkRawNext( txt:str ) mut : &LnsTypes.Token {
      let token = self.tokenizer.getTokenNoErr( false );
      if token.txt == txt {
         return token;
      }
      print(
         "%d:%d: Illegal token. expects '%s' but '%s'"
         ( token.pos.lineNo, token.pos.column, txt, token.txt) );
      os.exit( 1 );
   }
   
   pub fn getToken() mut : &LnsTypes.Token {
      let token = self.tokenizer.getTokenNoErr(false);
      switch token.txt {
         case "#" {
            let nextToken = self.tokenizer.getTokenNoErr(false);
            if nextToken.txt == "+" and nextToken.consecutive {
               return new LnsTypes.Token( .Dlmt, "#+", token.pos, false, nil );
            }
            self.tokenizer.pushback();
         }
         case ":" {
            let nextToken = self.tokenizer.getTokenNoErr( false );
            if nextToken.txt == ":" {
               self.checkRawNext( "=" );
               return new LnsTypes.Token( .Dlmt, "::=", token.pos, false, nil );
            } else {
               self.tokenizer.pushback();
            }
         }
         case "<" {
            if self.inEbnf {
               let nextToken = self.tokenizer.getTokenNoErr( false );
               self.checkRawNext( ">" );
               return new LnsTypes.Token(
                  .Type, "<%s>" (nextToken.txt), token.pos, false, nil );
            }
         }
      }
      return token;
   }

   pub fn checkNext( txt:str ) mut : &LnsTypes.Token {
      let token = self.getToken();
      if token.txt == txt {
         return token;
      }
      print(
         "%d:%d: Illegal token. expects '%s' but '%s'"
         ( token.pos.lineNo, token.pos.column, txt, token.txt) );
      os.exit( 1 );
   }

   pub fn pushBack( token:&LnsTypes.Token ) mut {
      self.tokenizer.pushbackToken( token );
   }

   pub fn skipLine( curToken:&LnsTypes.Token ) mut {
      let mut token = self.getToken();
      while token.pos.lineNo == curToken.pos.lineNo {
         token = self.getToken();
      }
      self.pushBack( token );
   }
}


pub class CodeTokenizer {
   let tokenizer:LnsTokenizer.Tokenizer;
   let mut tokenList:List<&LnsTypes.Token>;
   let mut commonList:__List<&LnsTypes.Token>;
   let pushbackedList:List<&LnsTypes.Token>;

   pub fn __init(tokenizer:LnsTokenizer.Tokenizer) {
      self.tokenizer = tokenizer;
      self.tokenList = [];
      self.commonList = [];
      self.pushbackedList = [];
   }

   pub fn getToken() mut : &LnsTypes.Token {
      if #self.pushbackedList > 0 {
         let token = self.pushbackedList[ #self.pushbackedList ];
         self.pushbackedList.remove(##);
         self.tokenList.insert( token );
         return token;
      }

      fn process( token:&LnsTypes.Token ) : &LnsTypes.Token {
         let result;
         if #self.commonList == 0 {
            result = token;
         } else {
            let newToken = new LnsTypes.Token(
               token.kind, token.txt, token.pos, token.consecutive, self.commonList );
            self.commonList = [];
            result = newToken;
         }
         self.tokenList.insert( result );
         return result;
      }
      
      while true {
         let! token = self.tokenizer.getToken() {
            token = LnsTypes.noneToken;
         };
         switch token.kind {
            case .Eof {
               return process(token);
            }
            case .Cmnt {
               self.commonList.insert( token );
            }
            default {
               return process(token);
            }
         } 
      }
   }

   pub fn pushback() mut {
      print( __func__, #self.tokenList, self.tokenList[ #self.tokenList ].txt );
      self.pushbackedList.insert( self.tokenList[ #self.tokenList ] );
      self.tokenList.remove( ## );
   }
}

pub proto class RuleList;
pub proto interface Core;
proto class ElementCore extend (Core);

pub class EbnfCtrl {
   // element 名 → RuleList マップ
   let element2ruleList:Map<str,&RuleList>;
   let allElementSet:Set<str>;
   let elementName2ElementCore:Map<str,&ElementCore>;

   pub fn __init() {
      self.element2ruleList = {};
      self.allElementSet = (@);
      self.elementName2ElementCore = {};
   }

   pub fn getRuleList( name:str ) : &RuleList! {
      return self.element2ruleList[ name ];
   }
}


pub proto class CodeGenerator;

pub interface CodeCore {
   pub fn outputCode( stream:CodeGenerator );
   pub fn pushback(tokenizer:CodeTokenizer);
}

pub class CodeGenerator {
   let stream:oStream;
   let tokenList:List<&LnsTypes.Token>;
   let mut prevToken:&LnsTypes.Token;

   pub fn __init( stream:oStream ) {
      self.stream = stream;
      self.tokenList = [];
      self.prevToken = LnsTypes.noneToken;
   }
   
   
   pub fn output( codeCore:CodeCore ) mut {
      codeCore.outputCode( self );
   }
   pub fn outputElement( elementName:str!, codeCoreList:&List<&CodeCore> ) mut {
      foreach core in codeCoreList {
         core.outputCode( self );
      }
   }
   pub fn outputToken( token:&LnsTypes.Token ) mut {
      if self.prevToken.pos.lineNo ~= token.pos.lineNo {
         self.stream.write( "\n" );
      } else {
         if not token.consecutive {
            self.stream.write( " " );
         }
      }
      self.stream.write( token.txt );
      self.tokenList.insert( token );
      self.prevToken = token;
   }
}


pub alge ParseCodeRet {
   /** EOF */
   Eof,
   /** 不適合 */
   Unmatch,
   /** 省略 */
   Abbr,
   /** 検出 */
   Detect(&CodeCore),
}

/**
ebnf の element を構成する一つの要素

以下の "if", <exp>, <block> の部分。

<hoge> ::= "if" <exp> <block>
*/
pub interface Core {
   pub fn getKeyword( ctrl:&EbnfCtrl, index:int, usedCoreSet:Set<&Core>): &List<str>;
   pub fn parseCode(
      ctrl:&EbnfCtrl, tokenizer:CodeTokenizer, usedElementSet:Set<&str>):ParseCodeRet;
}

pub class CodeCoreStat extend (CodeCore) {
   let list:&List<&LnsTypes.Token>;
   pub fn outputCode( stream:CodeGenerator ) {
      foreach token in self.list {
         stream.outputToken( token );
      }
   }
   pub fn pushback(tokenizer:CodeTokenizer) {
      foreach _ in self.list {
         tokenizer.pushback();
      }
   }
}


pub class CodeCoreBuiltin extend (CodeCore) {
   let token:&LnsTypes.Token;
   pub fn outputCode( stream:CodeGenerator ) {
      stream.outputToken( self.token );
   }
   pub fn pushback(tokenizer:CodeTokenizer) {
      tokenizer.pushback();
   }
}

pub class CodeCoreToken extend (CodeCore) {
   let core:&Core;
   let token:&LnsTypes.Token;
   pub fn outputCode( stream:CodeGenerator ) {
      stream.outputToken( self.token );
   }
   pub fn pushback(tokenizer:CodeTokenizer) {
      print( __func__, self.token.txt );
      tokenizer.pushback();
   }
}
pub class CodeCoreList extend (CodeCore) {
   let elementName:str!;
   let codeCoreList:&List<&CodeCore>;
   pub fn outputCode( stream:CodeGenerator ) {
      stream.outputElement( self.elementName, self.codeCoreList );
   }
   pub fn pushback(tokenizer:CodeTokenizer) {
      print( __func__, self.elementName );
      for index = #self.codeCoreList, 1, -1 {
         let codeCore = self.codeCoreList[ index ];
         codeCore.pushback( tokenizer );
      }
   }
}
pub class CodeSyntax {
   let ruleList:&RuleList;
   let codeCoreList:&List<&CodeCore>;
}



/**
ebnf の文

以下の右辺の部分。

<hoge> ::= "if" <exp> <block> 
*/
class Rule {
   let elementName:str!;
   let coreList:&List<&Core>;

   pub fn __init( elementName:str!, coreList:&List<&Core> ) {
      self.elementName = elementName;
      self.coreList = coreList;
   }

   pub fn getKeyword( ctrl:&EbnfCtrl, index:int, usedCoreSet:Set<&Core>): &List<str> {
      let mut targetIndex = index;
      let mut curIndex = 1;
      for coreIndex = 1, #self.coreList {
         let core = self.coreList[ coreIndex ];
         while curIndex <= targetIndex {
            let mut coreSet = usedCoreSet.clone();
            let list = core.getKeyword( ctrl, curIndex, coreSet );
            if #list == 0 {
               break;
            }
            if curIndex == targetIndex {
               return list;
            }
            curIndex = curIndex + 1;
         }
         targetIndex = targetIndex - curIndex + 1;
         curIndex = 1;
      }
      return [];
   }

   pub fn parseCode(
      ctrl:&EbnfCtrl, tokenizer:CodeTokenizer, usedElementSet:Set<&str> ) : ParseCodeRet
   {
      let codeCoreList:List<&CodeCore> = [];
      foreach core, index in self.coreList {
         print( __func__, index, #self.coreList ); 
         _match core.parseCode(ctrl, tokenizer, usedElementSet ) {
            case .Eof {
               return .Eof;
            }
            case .Unmatch {
               for coreIndex = #codeCoreList, 1, -1 {
                  let codeCore = codeCoreList[ coreIndex ];
                  codeCore.pushback( tokenizer );
               }
               return .Unmatch;
            }
            case .Abbr {
            }
            case .Detect( codeCore ) {
               codeCoreList.insert( codeCore );
            }
         }
      }
      if #codeCoreList == 0 {
         return .Abbr;
      }
      return .Detect( new CodeCoreList( self.elementName, codeCoreList ) );
   }
}

/**
ebnf の文。 or で括られた Rule をまとめたもの。


以下の <subfile_owner> と <subfile_sub> の部分。

  <subfile> ::= <subfile_owner> | <subfile_sub>
*/
pub class RuleList {
   let elementName:str;
   let list:&List<&Rule>;
   pub fn getKeyword( ctrl:&EbnfCtrl, index:int, usedCoreSet:Set<&Core>): &List<str> {
      let list:List<str> = [];
      foreach rule in self.list {
         foreach keyword in rule.getKeyword( ctrl, index, usedCoreSet ) {
            list.insert( keyword );
         }
      }
      return list;
   }
   pub fn parseCode(
      ctrl:&EbnfCtrl, tokenizer:CodeTokenizer, usedElementSet:Set<&str> ) : ParseCodeRet
   {
      // if usedElementSet.has( self.elementName ) {
      //    print( __func__, "already use", self.elementName );
      //    return .Unmatch;
      // }
      //usedElementSet.add( self.elementName );


      let mut result = ParseCodeRet.Unmatch;
      
      foreach rule, index in self.list {
         print( "check -- ", index, #self.list, self.elementName );
         _match rule.parseCode(ctrl, tokenizer, usedElementSet ) {
            case .Eof {
               return .Eof;
            }
            case .Unmatch {
            }
            case .Abbr {
               result = .Abbr;
               // error( "illegal abbr -- %s" ( self.elementName ) );
            }
            case .Detect( codeCore ) {
               print( "detect -- ", self.elementName );
               return .Detect( codeCore );
            }
         }
      }
      print( result.$_txt, " -- ", self.elementName );
      return result;
   }
}

/**
Token を Core として管理するクラス

以下の "if" の部分。

<hoge> ::= "if" <exp> <block>
 */
class TokenCore extend (Core) {
   let token:&LnsTypes.Token {pub};
   let rawTxt:str;
   pub fn __init( token:&LnsTypes.Token ) {
      self.token = token;
      self.rawTxt = token.getExcludedDelimitTxt();
   }
   pub fn getKeyword( ctrl:&EbnfCtrl, index:int, usedCoreSet:Set<&Core>): &List<str> {
      if index ~= 1 {
         return [];
      }
      return [ self.rawTxt ];
   }
   pub fn parseCode(
      ctrl:&EbnfCtrl, tokenizer:CodeTokenizer, usedElementSet:Set<&str>):ParseCodeRet
   {
      let token = tokenizer.getToken();
      print( __func__, self.token.txt, token.txt, token.kind.$_txt );
      if token.txt == self.rawTxt {
         print( __func__, "detect", token.txt,
                "%d:%d" (token.pos.lineNo, token.pos.column ) );
         return .Detect( new CodeCoreToken( self, token ) );
      }
      tokenizer.pushback();
      if token.kind == .Eof {
         return .Eof;
      }
      return .Unmatch;
   }
}

/**
element を Core にするクラス。

以下の <exp>, <block> の部分。

<hoge> ::= "if" <exp> <block>
*/
class ElementCore extend (Core) {
   let elementName:str;
   pri fn __init( elementName:str ) {
      self.elementName = elementName;
   }
   pub static fn create(
      elementName2ElementCore:Map<str,&ElementCore>, elementName:str ) : &ElementCore
   {
      let! core = elementName2ElementCore[ elementName ] {
         core = new ElementCore( elementName );
         elementName2ElementCore[ elementName ] = core;
      };
      return core;
   }
   pub fn getKeyword( ctrl:&EbnfCtrl, index:int, usedCoreSet:Set<&Core>): &List<str> {
      if not usedCoreSet.has( self ) {
         usedCoreSet.add( self );
         if! let ruleList = ctrl.getRuleList( self.elementName ) {
            return ruleList.getKeyword( ctrl, index, usedCoreSet );
         }
      } else {
         print( "already:", self.elementName );
      }
      return [];
   }
   pub fn parseCode(
      ctrl:&EbnfCtrl, tokenizer:CodeTokenizer, usedElementSet:Set<&str>):ParseCodeRet
   {
      switch self.elementName {
         case "<shebang>" {
            let token = tokenizer.getToken();
            if token.kind == .Sheb {
               return .Detect( new CodeCoreBuiltin( token ) );
            }
            tokenizer.pushback();
            return .Unmatch;
         }
      }
      
      if! let ruleList = ctrl.getRuleList( self.elementName ) {
         return ruleList.parseCode( ctrl, tokenizer, usedElementSet );
      }
      let token = tokenizer.getToken();
      switch self.elementName {
         case "<sym>" {
            switch token.kind {
               case .Symb, .Type, .Kywd {
                  print( __func__, "detect", self.elementName );
                  return .Detect( new CodeCoreBuiltin( token ) );
               }
            }
         }
         case "<literal_str>" {
            if token.kind == .Str {
               print( __func__, "detect", self.elementName );
               return .Detect( new CodeCoreBuiltin( token ) );
            }
         }
         case "<literal_int>" {
            if token.kind == .Int {
               print( __func__, "detect", self.elementName );
               return .Detect( new CodeCoreBuiltin( token ) );
            }
         }
         case "<literal_real>" {
            if token.kind == .Real {
               print( __func__, "detect", self.elementName );
               return .Detect( new CodeCoreBuiltin( token ) );
            }
         }
         case "<literal_char>" {
            if token.kind == .Char {
               print( __func__, "detect", self.elementName );
               return .Detect( new CodeCoreBuiltin( token ) );
            }
         }
         case "<token>" {
            if token.txt ~= ";" {
               print( __func__, "detect", self.elementName );
               return .Detect( new CodeCoreBuiltin( token ) );
            }
         }
         case "<stat>" {
            let list:List<&LnsTypes.Token> = [];
            let mut workToken = token;
            let mut depth = 1;
            while true {
               if workToken.kind == .Eof {
                  return .Eof;
               }
               switch workToken.txt {
                  case "}" {
                     depth = depth - 1;
                     if depth == 0 {
                        break;
                     }
                  }
                  case "{", "`{" {
                     depth = depth + 1;
                  }
               }
               list.insert( workToken );
               workToken = tokenizer.getToken();
            }
            tokenizer.pushback();
            return .Detect( new CodeCoreStat( list ));
         }
         default {
            print( "unknown -- ", self.elementName );
         }
      }
      tokenizer.pushback();
      return .Unmatch;
   }
}
enum RuleKind {
   // 0 or 1
   Option,
   // 0 以上
   Repeat,
}
class RuleCore extend (Core) {
   let kind:RuleKind;
   let rule:&Rule;
   pub fn getKeyword( ctrl:&EbnfCtrl, index:int, usedCoreSet:Set<&Core>): &List<str> {
      return [];
   }
   pub fn parseCode(
      ctrl:&EbnfCtrl, tokenizer:CodeTokenizer, usedElementSet:Set<&str> ) : ParseCodeRet
   {
      let list:List<&CodeCore> = [];
      while true {
         let result = self.rule.parseCode( ctrl, tokenizer, usedElementSet );
         _match result {
            case .Eof {
               if #list == 0 {
                  return .Abbr;
               } else {
                  return .Detect( new CodeCoreList( nil, list ) );
               }
            }
            case .Unmatch {
               if #list == 0 {
                  return .Abbr;
               } else {
                  return .Detect( new CodeCoreList( nil, list ) );
               }
            }
            case .Abbr {
               if #list == 0 {
                  return .Abbr;
               } else {
                  return .Detect( new CodeCoreList( nil, list ) );
               }
            }
            case .Detect( codeCore ) {
               print( __func__, self.kind.$_txt  );
               if self.kind == .Option {
                  return result;
               }
               list.insert( codeCore );
            }
         }
      }
   }
}

fn isElement( token:&LnsTypes.Token ) : bool {
   return token.kind == .Type and token.txt[1] == ?<;
}


class DeclTokenizer {
   let tokenizer:EbnfTokenizer;
   let mut prevToken:LnsTokenizer.Token;
   let allElementSet:Set<str>;

   pub fn __init( tokenizer:EbnfTokenizer, prevToken:LnsTokenizer.Token,
                  allElementSet:Set<str> )
   {
      self.tokenizer = tokenizer;
      self.prevToken = prevToken;
      self.allElementSet = allElementSet;
   }
   
   pub fn getToken() mut : &LnsTypes.Token! {
      let nextToken = self.tokenizer.getToken();
      if nextToken.kind == .Eof {
         return nil;
      }
      if nextToken.pos.lineNo ~= self.prevToken.pos.lineNo {
         if nextToken.txt == "#+" {
            self.tokenizer.pushBack( nextToken );
            return nil;
         }
         let checkToken = self.tokenizer.getToken();
         self.tokenizer.pushBack( checkToken );
         if checkToken.txt == "::=" {
            self.tokenizer.pushBack( nextToken );
            return nil;
         }
      }
      if isElement( nextToken ) {
         self.allElementSet.add( nextToken.txt );
      }
      self.prevToken = nextToken;
      return nextToken;
   }

   pub fn err( mess:str ) {
      print( "%d:%d:" (self.prevToken.pos.lineNo, self.prevToken.pos.column), mess );
      os.exit( 1 );
   }
}

fn EbnfCtrl.processRule(
   elementName:str!, tokenizer:DeclTokenizer, termTxt:str! ) mut : &Rule!, str
{
   let mut coreList:List<&Core> = [];
   let mut endsTerm = false;
   while true {
      let! token = tokenizer.getToken() {
         break;
      };
      switch token.txt {
         case "[" {
            let optRule = self.processRule( nil, tokenizer, "]" )!;
            coreList.insert( new RuleCore( .Option, optRule ) );
         }
         case "{" {
            let optRule = self.processRule( nil, tokenizer, "}" )!;
            coreList.insert( new RuleCore( .Repeat, optRule ) );
         }
         case "|" {
            break;
         }
         case termTxt {
            endsTerm = true;
            break;
         }
         default {
            if isElement( token ) {
               coreList.insert(
                  ElementCore.create( self.elementName2ElementCore,token.txt ) );
            } else {
               coreList.insert( new TokenCore( token ) );
            }
         }
      }
   }
   if #coreList == 0 {
      return nil, "coreList is 0";
   }
   if termTxt and not endsTerm {
      return nil, "not end term -- %s" (termTxt) ;
   }
   return new Rule( elementName, coreList ), "";
}


local fn EbnfCtrl.processDecl(
   tokenizer:EbnfTokenizer, symbolToken:&LnsTypes.Token ) mut : &RuleList
{
   let elementName = symbolToken.txt;
   print( __func__, elementName );
   tokenizer.checkNext( "::=" );

   let mut list:List<&Rule> = [];
   while true {
      let mut declTokenizer = new DeclTokenizer( tokenizer, symbolToken, self.allElementSet );
      let rule, mess = self.processRule( elementName, declTokenizer, nil );
      when! rule {
         list.insert( rule );
      } else {
         if #list == 0 {
            declTokenizer.err( mess );
         }
         break;
      }
   }
   let ruleList = new RuleList( elementName, list );
   self.element2ruleList[ elementName ] = ruleList;
   return ruleList;
}

pub fn EbnfCtrl.dump() {
   print( "=================" );
   forsort element in self.allElementSet {
      print( self.element2ruleList[ element ] ~= nil, element );
   }
}

pub fn EbnfCtrl.parse( elementName:str, tokenizer:CodeTokenizer ) {
   let! ruleList = self.element2ruleList[ elementName ] {
      print( "error parse" );
      return;
   };
   _match ruleList.parseCode( self, tokenizer, (@) ) {
      case .Eof {
         print( "eof" );
      }
      case .Unmatch {
         print( "unmatch" );
      }
      case .Abbr {
         let pos = tokenizer.getToken().pos;
         print( "illegal", pos.lineNo, pos.column );
      }
      case .Detect( codeCore ) {
         print( "outputCode" );
         codeCore.outputCode( new CodeGenerator( io.stdout ) );
         io.stdout.flush();
         print( "" );
         let token = tokenizer.getToken();
         let pos = token.pos;
         print( "%d:%d" (pos.lineNo, pos.column), token.kind.$_txt );
      }
   }
}

pub fn analyze_ebnf( tokenizer:EbnfTokenizer ) : &EbnfCtrl {

   let mut ebnfCtrl = new EbnfCtrl();
   
   while true {
      let token = tokenizer.getToken();
      if token.kind == .Eof {
         break;
      }
      if token.txt == "#" {
         tokenizer.skipLine( token );
      } elseif token.txt == "#+" {
         let nextToken = tokenizer.getToken();
         switch nextToken.txt.upper() {
            case "BEGIN_SRC" {
               tokenizer.set_inEbnf( true );
            }
            case "END_SRC" {
               tokenizer.set_inEbnf( false );
            }
         }
         tokenizer.skipLine( token );
      } elseif tokenizer.$inEbnf {
         if isElement( token ) {
            ebnfCtrl.processDecl( tokenizer, token );
         }
      }
   }

   return ebnfCtrl;
}
