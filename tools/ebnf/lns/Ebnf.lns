// -*- coding: utf-8 -*-

_lune_control default_async_all;

import go/github:com.ifritJP.LuneScript.src.lune.base.Tokenizer as LnsTokenizer;
import go/github:com.ifritJP.LuneScript.src.lune.base.Types as LnsTypes;

fn log(...) {
   print( ... );
}


pub class EbnfTokenizer {
   let tokenizer:LnsTokenizer.PushbackTokenizer;
   let mut inEbnf:bool {pub,pub};

   fn __init( tokenizer:LnsTokenizer.Tokenizer ) {
      self.tokenizer = new LnsTokenizer.DefaultPushbackTokenizer( tokenizer );
      self.inEbnf = false;
   }

   pub static fn create() : EbnfTokenizer {
      let path = "../../../ifritJP.github.io/hugo/content/LuneScript/ebnf.ja.org";
      let mut tokenizer = LnsTokenizer.StreamTokenizer.create(
         LnsTypes.TokenizerSrc.LnsPath( nil, path, "ebnf", nil ), false, nil, nil );
      return new EbnfTokenizer( tokenizer );
   }

   pri fn checkRawNext( txt:str ) mut : &LnsTypes.Token {
      let token = self.tokenizer.getTokenNoErr( false );
      if token.txt == txt {
         return token;
      }
      log(
         "%d:%d: Illegal token. expects '%s' but '%s'"
         ( token.pos.lineNo, token.pos.column, txt, token.txt) );
      os.exit( 1 );
   }
   
   pub fn getToken() mut : &LnsTypes.Token {
      let token = self.tokenizer.getTokenNoErr(false);
      switch token.txt {
         case "#" {
            let nextToken = self.tokenizer.getTokenNoErr(false);
            if nextToken.txt == "+" and nextToken.consecutive {
               return new LnsTypes.Token( .Dlmt, "#+", token.pos, false, nil );
            }
            self.tokenizer.pushback();
         }
         case ":" {
            let nextToken = self.tokenizer.getTokenNoErr( false );
            if nextToken.txt == ":" {
               self.checkRawNext( "=" );
               return new LnsTypes.Token( .Dlmt, "::=", token.pos, false, nil );
            } else {
               self.tokenizer.pushback();
            }
         }
         case "<" {
            if self.inEbnf {
               let nextToken = self.tokenizer.getTokenNoErr( false );
               self.checkRawNext( ">" );
               return new LnsTypes.Token(
                  .Type, "<%s>" (nextToken.txt), token.pos, false, nil );
            }
         }
      }
      return token;
   }

   pub fn checkNext( txt:str ) mut : &LnsTypes.Token {
      let token = self.getToken();
      if token.txt == txt {
         return token;
      }
      log(
         "%d:%d: Illegal token. expects '%s' but '%s'"
         ( token.pos.lineNo, token.pos.column, txt, token.txt) );
      os.exit( 1 );
   }

   pub fn pushBack( token:&LnsTypes.Token ) mut {
      self.tokenizer.pushbackToken( token );
   }

   pub fn skipLine( curToken:&LnsTypes.Token ) mut {
      let mut token = self.getToken();
      while token.pos.lineNo == curToken.pos.lineNo {
         token = self.getToken();
      }
      self.pushBack( token );
   }
}


pub class CodeTokenizer {
   /** コードの tokenizer 元 */
   let tokenizer:LnsTokenizer.Tokenizer;
   /** 読み込み済みの token のリスト */
   let mut tokenList:List<&LnsTypes.Token>;
   /**
   tokenList のアクセス位置。
   pushBack したときは、この位置を前後に動かす。
   次の条件のときに、tokenList から token を読み出す。
   1 <= tokenIndex <= #tokenList 
   */ 
   let mut tokenIndex:int;
   /** コメントリスト。 次のコメント以外の token に付加する */
   let mut commentList:__List<&LnsTypes.Token>;

   pub fn __init(tokenizer:LnsTokenizer.Tokenizer) {
      self.tokenizer = tokenizer;
      self.tokenList = [];
      self.commentList = [];
      self.tokenIndex = 1;
   }

   /**
   トークン取得

コメントがある場合は、コメントを読み飛ばして次のトークンを取得する。
   */
   pub fn getToken() mut : &LnsTypes.Token {
      if self.tokenIndex <= #self.tokenList {
         let token = self.tokenList[ self.tokenIndex ];
         self.tokenIndex = self.tokenIndex + 1;
         return token;
      }

      fn process( token:&LnsTypes.Token ) : &LnsTypes.Token {
         let result;
         if #self.commentList == 0 {
            result = token;
         } else {
            let newToken = new LnsTypes.Token(
               token.kind, token.txt, token.pos, token.consecutive, self.commentList );
            self.commentList = [];
            result = newToken;
         }
         self.tokenList.insert( result );
         self.tokenIndex = #self.tokenList + 1;
         return result;
      }
      
      while true {
         let! token = self.tokenizer.getToken() {
            token = LnsTypes.noneToken;
         };
         switch token.kind {
            case .Eof {
               return process(token);
            }
            case .Cmnt {
               self.commentList.insert( token );
            }
            default {
               return process(token);
            }
         } 
      }
   }

   /**
   現在のトークンを pushback する
   */
   pub fn pushback() mut {
      log( __func__, #self.tokenList, self.tokenIndex );
      if self.tokenIndex <= 1 {
         error( "illegal tokenIndex" );
      }
      self.tokenIndex = self.tokenIndex - 1;
   }

   pub fn peekToken() mut : &LnsTypes.Token {
      if self.tokenIndex <= #self.tokenList {
         let token = self.tokenList[ self.tokenIndex ];
         return token;
      }
      let token = self.getToken();
      self.pushback();
      return token;
   }
}

pub proto class RuleList;
pub proto interface Core;
proto class ElementCore extend (Core);
pub proto class CodeGenerator;

pub interface CodeCore {
   pub fn outputCode( stream:CodeGenerator );
   pub fn pushback(tokenizer:CodeTokenizer);
}

pub class CodeGenerator {
   let stream:oStream;
   let tokenList:List<&LnsTypes.Token>;
   let mut prevToken:&LnsTypes.Token;

   pub fn __init( stream:oStream ) {
      self.stream = stream;
      self.tokenList = [];
      self.prevToken = LnsTypes.noneToken;
   }
   
   
   pub fn output( codeCore:CodeCore ) mut {
      codeCore.outputCode( self );
   }
   pub fn outputElement( elementName:str!, codeCoreList:&List<&CodeCore> ) mut {
      foreach core in codeCoreList {
         core.outputCode( self );
      }
   }
   pub fn outputToken( token:&LnsTypes.Token ) mut {
      if self.prevToken.pos.lineNo ~= token.pos.lineNo {
         self.stream.write( "\n" );
      } else {
         if not token.consecutive {
            self.stream.write( " " );
         }
      }
      self.stream.write( token.txt );
      self.tokenList.insert( token );
      self.prevToken = token;
   }
}


pub alge ParseCodeRet {
   /** EOF */
   Eof,
   /** 不適合 */
   Unmatch,
   /** 省略 */
   Abbr,
   /** 検出 */
   Detect(&CodeCore),
}


pub class Candidate {
   let txtSet:Set<str>;
   let tokenKindSet:Set<LnsTypes.TokenKind>;
   let mut canAbbr:bool {pub};
   let mut any:bool {pub};

   pub fn __init(canAbbr:bool, any:bool ) {
      self.canAbbr = canAbbr;
      self.txtSet = (@);
      self.tokenKindSet = (@);
      self.any = any;
   }

   pub fn addTxt( txt:str ) mut {
      self.txtSet.add( txt) ;
   }
   pub fn addTokenKind( kind:LnsTypes.TokenKind ) mut {
      self.tokenKindSet.add( kind ) ;
   }
   pub fn add( other:&Candidate, validAbbr:bool ) mut {
      self.txtSet.or( other.txtSet );
      self.tokenKindSet.or( other.tokenKindSet );
      if validAbbr {
         self.canAbbr = self.canAbbr and other.canAbbr;
      }
      self.any = self.any and other.any;
   }

   pub fn hasTxt( txt:str ) : bool {
      return self.txtSet.has( txt );
   }
   pub fn hasKind( kind:LnsTypes.TokenKind ) : bool {
      return self.tokenKindSet.has( kind );
   }
   pub fn has( token:&LnsTypes.Token ) : bool {
      return self.hasTxt( token.txt ) or self.hasKind( token.kind );
   }

   pub fn dump( stream:oStream ) {
      stream.write( "txt: " );
      foreach txt in self.txtSet {
         stream.write( "%s, " (txt) );
      }
      stream.write( "\n" );
      stream.write( "kind: " );
      foreach kind in self.tokenKindSet {
         stream.write( "%s, " (kind.$_txt));
      }
      stream.write( "\n" );
      stream.write( "abbr: %s, any: %s\n" (self.canAbbr, self.any ) );
   }
}


pub proto class EbnfCtrl;

/**
ebnf の element を構成する一つの要素

以下の "if", <exp>, <block> の部分。

<hoge> ::= "if" <exp> <block>
*/
pub interface Core {
   pub fn setupCandidate( ctrl:&EbnfCtrl, usedCoreSet:Set<&Core>) mut : &Candidate;
   pub fn get_candidate(): &Candidate;
   pub fn parseCode(
      ctrl:&EbnfCtrl, tokenizer:CodeTokenizer, usedElementSet:Set<&str>):ParseCodeRet;
}

pub class CodeCoreStat extend (CodeCore) {
   let list:&List<&LnsTypes.Token>;
   pub fn outputCode( stream:CodeGenerator ) {
      foreach token in self.list {
         stream.outputToken( token );
      }
   }
   pub fn pushback(tokenizer:CodeTokenizer) {
      foreach _ in self.list {
         tokenizer.pushback();
      }
   }
}


pub class CodeCoreBuiltin extend (CodeCore) {
   let token:&LnsTypes.Token;
   pub fn outputCode( stream:CodeGenerator ) {
      stream.outputToken( self.token );
   }
   pub fn pushback(tokenizer:CodeTokenizer) {
      tokenizer.pushback();
   }
}

pub class CodeCoreToken extend (CodeCore) {
   let core:&Core;
   let token:&LnsTypes.Token;
   pub fn outputCode( stream:CodeGenerator ) {
      stream.outputToken( self.token );
   }
   pub fn pushback(tokenizer:CodeTokenizer) {
      log( __func__, self.token.txt );
      tokenizer.pushback();
   }
}
pub class CodeCoreList extend (CodeCore) {
   let elementName:str!;
   let codeCoreList:&List<&CodeCore>;
   pub fn outputCode( stream:CodeGenerator ) {
      stream.outputElement( self.elementName, self.codeCoreList );
   }
   pub fn pushback(tokenizer:CodeTokenizer) {
      log( __func__, self.elementName );
      for index = #self.codeCoreList, 1, -1 {
         let codeCore = self.codeCoreList[ index ];
         codeCore.pushback( tokenizer );
      }
   }
}
pub class CodeSyntax {
   let ruleList:&RuleList;
   let codeCoreList:&List<&CodeCore>;
}



/**
ebnf の文

以下の右辺の部分。

<hoge> ::= "if" <exp> <block> 
*/
class Rule {
   let elementName:str!;
   let coreList:&List<Core>;

   pub fn __init( elementName:str!, coreList:&List<Core> ) {
      self.elementName = elementName;
      self.coreList = coreList;
   }

   pub fn setupCandidate( ctrl:&EbnfCtrl, usedCoreSet:Set<&Core>) mut : &Candidate {
      let mut candidate = new Candidate( true, true );
      foreach core in self.coreList {
         let mut coreSet = usedCoreSet.clone();
         let work = core.setupCandidate( ctrl, coreSet );
         candidate.add( work, true );
         if not work.$canAbbr {
            break;
         }
      }
      return candidate;
   }

   pub fn parseCode(
      ctrl:&EbnfCtrl, tokenizer:CodeTokenizer, usedElementSet:Set<&str> ) : ParseCodeRet
   {
      let codeCoreList:List<&CodeCore> = [];
      foreach core, index in self.coreList {
         log( __func__, index, #self.coreList ); 
         _match core.parseCode(ctrl, tokenizer, usedElementSet ) {
            case .Eof {
               return .Eof;
            }
            case .Unmatch {
               for coreIndex = #codeCoreList, 1, -1 {
                  let codeCore = codeCoreList[ coreIndex ];
                  codeCore.pushback( tokenizer );
               }
               return .Unmatch;
            }
            case .Abbr {
            }
            case .Detect( codeCore ) {
               codeCoreList.insert( codeCore );
            }
         }
      }
      if #codeCoreList == 0 {
         return .Abbr;
      }
      return .Detect( new CodeCoreList( self.elementName, codeCoreList ) );
   }
}

/**
ebnf の文。 or で括られた Rule をまとめたもの。


以下の <subfile_owner> と <subfile_sub> の部分。

  <subfile> ::= <subfile_owner> | <subfile_sub>
*/
pub class RuleList {
   let elementName:str;
   let list:&List<Rule>;
   let candidate:Candidate {pub&};

   local fn __init( elementName:str, list:&List<Rule> ) {
      self.elementName = elementName;
      self.list = list;
      self.candidate = new Candidate( true, true );
   }
   
   pub fn setupCandidate( ctrl:&EbnfCtrl, usedCoreSet:Set<&Core>) mut : &Candidate {
      foreach rule in self.list {
         self.candidate.add( rule.setupCandidate( ctrl, usedCoreSet ), true );
      }
      return self.candidate;
   }
   pub fn parseCode(
      ctrl:&EbnfCtrl, tokenizer:CodeTokenizer, usedElementSet:Set<&str> ) : ParseCodeRet
   {
      if not self.candidate.has( tokenizer.peekToken() ) {
         log( __func__, "not found candidate -- ",
              tokenizer.peekToken().txt, tokenizer.peekToken().kind.$_txt,
              self.elementName );
         if self.candidate.$canAbbr {
            return .Abbr;
         }
         return .Unmatch;
      }
      
      let mut result = ParseCodeRet.Unmatch;

      foreach rule, index in self.list {
         log( "check -- ", index, #self.list, self.elementName );
         _match rule.parseCode(ctrl, tokenizer, usedElementSet ) {
            case .Eof {
               return .Eof;
            }
            case .Unmatch {
            }
            case .Abbr {
               result = .Abbr;
               // error( "illegal abbr -- %s" ( self.elementName ) );
            }
            case .Detect( codeCore ) {
               log( "detect -- ", self.elementName );
               return .Detect( codeCore );
            }
         }
      }
      log( result.$_txt, " -- ", self.elementName );
      return result;
   }
}

/**
Token を Core として管理するクラス

以下の "if" の部分。

<hoge> ::= "if" <exp> <block>
 */
class TokenCore extend (Core) {
   let token:&LnsTypes.Token {pub};
   let rawTxt:str;
   let candidate:Candidate {pub&};
   
   pub fn __init( token:&LnsTypes.Token ) {
      self.token = token;
      self.rawTxt = token.getExcludedDelimitTxt();
      self.candidate = new Candidate( false, false );
      self.candidate.addTxt( self.rawTxt );
   }
   pub fn setupCandidate( ctrl:&EbnfCtrl, usedCoreSet:Set<&Core>) mut : &Candidate {
      return self.candidate;
   }
   pub fn parseCode(
      ctrl:&EbnfCtrl, tokenizer:CodeTokenizer, usedElementSet:Set<&str>):ParseCodeRet
   {
      let token = tokenizer.getToken();
      log( __func__, self.token.txt, token.txt, token.kind.$_txt );
      if token.txt == self.rawTxt {
         log( __func__, "detect", token.txt,
                "%d:%d" (token.pos.lineNo, token.pos.column ) );
         return .Detect( new CodeCoreToken( self, token ) );
      }
      tokenizer.pushback();
      if token.kind == .Eof {
         return .Eof;
      }
      return .Unmatch;
   }
}

class BuiltinTokenKindCore extend (Core) {
   let elementName:str;
   let tokenKindSet:__Set<LnsTypes.TokenKind>;
   let candidate:Candidate {pub&};
   
   pub fn __init( elementName:str, tokenKindSet:__Set<LnsTypes.TokenKind> ) {
      self.elementName = elementName;
      self.tokenKindSet = tokenKindSet;
      self.candidate = new Candidate( false, false );
      foreach kind in tokenKindSet {
         self.candidate.addTokenKind( kind );
      }
   }

   pub fn setupCandidate( ctrl:&EbnfCtrl, usedCoreSet:Set<&Core>) mut : &Candidate {
      return self.candidate;
   }
   pub fn parseCode(
      ctrl:&EbnfCtrl, tokenizer:CodeTokenizer, usedElementSet:Set<&str>):ParseCodeRet
   {
      let token = tokenizer.getToken();
      if self.tokenKindSet.has( token.kind ) {
         log( __func__, "detect", self.elementName );
         return .Detect( new CodeCoreBuiltin( token ) );
      }
      tokenizer.pushback();
      return .Unmatch;
   }
}

class BuiltinTokenCore extend (Core) {
   let elementName:str;
   let tokenTxtSet:__Set<str>;
   let candidate:Candidate {pub&};

   pub fn __init( elementName:str, tokenTxtSet:__Set<str> ) {
      self.elementName = elementName;
      self.tokenTxtSet = tokenTxtSet;
      self.candidate = new Candidate( false, false );
      foreach kind in tokenTxtSet {
         self.candidate.addTxt( kind );
      }
   }

   pub fn setupCandidate( ctrl:&EbnfCtrl, usedCoreSet:Set<&Core>) mut : &Candidate {
      return self.candidate;
   }
   pub fn parseCode(
      ctrl:&EbnfCtrl, tokenizer:CodeTokenizer, usedElementSet:Set<&str>):ParseCodeRet
   {
      let token = tokenizer.getToken();
      if not self.tokenTxtSet.has( token.txt ) {
         log( __func__, "detect", self.elementName );
         return .Detect( new CodeCoreBuiltin( token ) );
      }
      tokenizer.pushback();
      return .Unmatch;
   }
}

class BuiltinStatCore extend (Core) {
   let candidate:Candidate {pub&};

   pub fn __init() {
      self.candidate = new Candidate( false, true );
   }
   
   pub fn setupCandidate( ctrl:&EbnfCtrl, usedCoreSet:Set<&Core>) mut : &Candidate {
      return self.candidate;
   }
   pub fn parseCode(
      ctrl:&EbnfCtrl, tokenizer:CodeTokenizer, usedElementSet:Set<&str>):ParseCodeRet
   {
      let list:List<&LnsTypes.Token> = [];
      let mut depth = 1;
      while true {
         let mut workToken = tokenizer.getToken();
         if workToken.kind == .Eof {
            return .Eof;
         }
         switch workToken.txt {
            case "}" {
               depth = depth - 1;
               if depth == 0 {
                  break;
               }
            }
            case "{", "`{" {
               depth = depth + 1;
            }
         }
         list.insert( workToken );
      }
      tokenizer.pushback();
      return .Detect( new CodeCoreStat( list ));
   }
}





class BuiltinCore {
   local static let elementNameMap:Map<str,Core>;
   __init {
      BuiltinCore.elementNameMap = {};

      {
         let map:__Map<str,__Set<LnsTypes.TokenKind>> = {
            "<shebang>": (@ .Sheb),
            "<sym>": (@ .Symb, .Type, .Kywd, .Ope ),
            "<literal_str>": (@ .Str ),
            "<literal_int>": (@ .Int ),
            "<literal_real>": (@ .Real ),
            "<literal_char>": (@ .Char ),
         };
         foreach tokenKindSet, elementName in map {
            BuiltinCore.elementNameMap[ elementName ] = new BuiltinTokenKindCore(
               elementName, tokenKindSet );
         }
      }

      {
         let map:__Map<str,__Set<str>> = {
            "<token>": (@ ";"),
         };
         foreach tokenTxtSet, elementName in map {
            BuiltinCore.elementNameMap[ "<token>" ] = new BuiltinTokenCore(
               elementName, tokenTxtSet );
         }
      }
      BuiltinCore.elementNameMap[ "<stat>" ] = new BuiltinStatCore();
   }
   pub static fn get( elementName:str ) __noasync : Core! {
      return BuiltinCore.elementNameMap[ elementName ];
   }
}

pub class EbnfCtrl {
   // element 名 → RuleList マップ
   let element2ruleList:Map<str,RuleList>;
   let allElementSet:Set<str>;
   let elementName2Core:Map<str,Core>;

   pub fn __init() {
      self.element2ruleList = {};
      self.allElementSet = (@);
      self.elementName2Core = {};
   }

   pub fn getRuleList( name:str ) : RuleList! {
      return self.element2ruleList[ name ];
   }
}



/**
element を Core にするクラス。

以下の <exp>, <block> の部分。

<hoge> ::= "if" <exp> <block>
*/
class ElementCore extend (Core) {
   let elementName:str;
   let candidate:Candidate {pub&};
   pri fn __init( elementName:str ) {
      self.elementName = elementName;
      self.candidate = new Candidate( true, false );
   }
   pub static fn create(
      elementName2Core:Map<str,Core>, elementName:str ) : Core
   {
      let! mut core = elementName2Core[ elementName ] {
         __asyncLock {
            if! BuiltinCore.get( elementName ) {
               core = _exp;
            } else {
               core = new ElementCore( elementName );
            }
         }
         elementName2Core[ elementName ] = core;
      };
      return core;
   }
   pub fn setupCandidate( ctrl:&EbnfCtrl, usedCoreSet:Set<&Core>) mut : &Candidate {
      print( __func__, self.elementName );
      if not usedCoreSet.has( self ) {
         usedCoreSet.add( self );
         if! let mut ruleList = ctrl.getRuleList( self.elementName ) {
            self.candidate.add(ruleList.setupCandidate( ctrl, usedCoreSet ), true );
         } else {
            error( "unknown -- %s" (self.elementName) );
         }
      } else {
         log( "already:", self.elementName );
      }
      return self.candidate;
   }
   pub fn parseCode(
      ctrl:&EbnfCtrl, tokenizer:CodeTokenizer, usedElementSet:Set<&str>):ParseCodeRet
   {
      let! ruleList = ctrl.getRuleList( self.elementName ) {
         return .Unmatch;
      };
      return ruleList.parseCode( ctrl, tokenizer, usedElementSet );
   }
}
enum RuleKind {
   // 0 or 1
   Option,
   // 0 以上
   Repeat,
}
class RuleCore extend (Core) {
   let kind:RuleKind;
   let rule:Rule;
   let candidate:Candidate {pub&};

   local fn __init( kind:RuleKind, rule:Rule ) {
      self.kind = kind;
      self.rule = rule;
      self.candidate = new Candidate( true, false );
   }

   pub fn setupCandidate( ctrl:&EbnfCtrl, usedCoreSet:Set<&Core>) mut: &Candidate {
      self.candidate.add( self.rule.setupCandidate( ctrl, usedCoreSet ), false );
      return self.candidate;
   }
   pub fn parseCode(
      ctrl:&EbnfCtrl, tokenizer:CodeTokenizer, usedElementSet:Set<&str> ) : ParseCodeRet
   {
      let list:List<&CodeCore> = [];
      while true {
         let result = self.rule.parseCode( ctrl, tokenizer, usedElementSet );
         _match result {
            case .Eof {
               if #list == 0 {
                  return .Abbr;
               } else {
                  return .Detect( new CodeCoreList( nil, list ) );
               }
            }
            case .Unmatch {
               if #list == 0 {
                  return .Abbr;
               } else {
                  return .Detect( new CodeCoreList( nil, list ) );
               }
            }
            case .Abbr {
               if #list == 0 {
                  return .Abbr;
               } else {
                  return .Detect( new CodeCoreList( nil, list ) );
               }
            }
            case .Detect( codeCore ) {
               log( __func__, self.kind.$_txt  );
               if self.kind == .Option {
                  return result;
               }
               list.insert( codeCore );
            }
         }
      }
   }
}

fn isElement( token:&LnsTypes.Token ) : bool {
   return token.kind == .Type and token.txt[1] == ?<;
}


class DeclTokenizer {
   let tokenizer:EbnfTokenizer;
   let mut prevToken:LnsTokenizer.Token;
   let allElementSet:Set<str>;

   pub fn __init( tokenizer:EbnfTokenizer, prevToken:LnsTokenizer.Token,
                  allElementSet:Set<str> )
   {
      self.tokenizer = tokenizer;
      self.prevToken = prevToken;
      self.allElementSet = allElementSet;
   }
   
   pub fn getToken() mut : &LnsTypes.Token! {
      let nextToken = self.tokenizer.getToken();
      if nextToken.kind == .Eof {
         return nil;
      }
      if nextToken.pos.lineNo ~= self.prevToken.pos.lineNo {
         if nextToken.txt == "#+" {
            self.tokenizer.pushBack( nextToken );
            return nil;
         }
         let checkToken = self.tokenizer.getToken();
         self.tokenizer.pushBack( checkToken );
         if checkToken.txt == "::=" {
            self.tokenizer.pushBack( nextToken );
            return nil;
         }
      }
      if isElement( nextToken ) {
         self.allElementSet.add( nextToken.txt );
      }
      self.prevToken = nextToken;
      return nextToken;
   }

   pub fn err( mess:str ) {
      log( "%d:%d:" (self.prevToken.pos.lineNo, self.prevToken.pos.column), mess );
      os.exit( 1 );
   }
}

fn EbnfCtrl.processRule(
   elementName:str!, tokenizer:DeclTokenizer, termTxt:str! ) mut : Rule!, str
{
   let mut coreList:List<Core> = [];
   let mut endsTerm = false;
   while true {
      let! token = tokenizer.getToken() {
         break;
      };
      switch token.txt {
         case "[" {
            let mut optRule = self.processRule( nil, tokenizer, "]" )!;
            coreList.insert( new RuleCore( .Option, optRule ) );
         }
         case "{" {
            let mut optRule = self.processRule( nil, tokenizer, "}" )!;
            coreList.insert( new RuleCore( .Repeat, optRule ) );
         }
         case "|" {
            break;
         }
         case termTxt {
            endsTerm = true;
            break;
         }
         default {
            if isElement( token ) {
               coreList.insert(
                  ElementCore.create( self.elementName2Core,token.txt ) );
            } else {
               coreList.insert( new TokenCore( token ) );
            }
         }
      }
   }
   if #coreList == 0 {
      return nil, "coreList is 0";
   }
   if termTxt and not endsTerm {
      return nil, "not end term -- %s" (termTxt) ;
   }
   return new Rule( elementName, coreList ), "";
}


local fn EbnfCtrl.processDecl(
   tokenizer:EbnfTokenizer, symbolToken:&LnsTypes.Token ) mut : &RuleList
{
   let elementName = symbolToken.txt;
   log( __func__, elementName );
   tokenizer.checkNext( "::=" );

   self.allElementSet.add( elementName );

   let mut list:List<Rule> = [];
   while true {
      let mut declTokenizer = new DeclTokenizer( tokenizer, symbolToken, self.allElementSet );
      let mut rule, mess = self.processRule( elementName, declTokenizer, nil );
      when! rule {
         list.insert( rule );
      } else {
         if #list == 0 {
            declTokenizer.err( mess );
         }
         break;
      }
   }
   let mut ruleList = new RuleList( elementName, list );
   self.element2ruleList[ elementName ] = ruleList;
   return ruleList;
}

pub fn EbnfCtrl.dump() {
   log( "=================" );
   forsort element in self.allElementSet {
      let ruleList = self.element2ruleList[ element ];
      log( ruleList ~= nil, element );
      when! ruleList {
         ruleList.get_candidate().dump( io.stdout );
      }
   }
}

pub fn EbnfCtrl.parse( elementName:str, tokenizer:CodeTokenizer ) {
   let! ruleList = self.element2ruleList[ elementName ] {
      log( "error parse" );
      return;
   };
   _match ruleList.parseCode( self, tokenizer, (@) ) {
      case .Eof {
         log( "eof" );
      }
      case .Unmatch {
         log( "unmatch" );
      }
      case .Abbr {
         let pos = tokenizer.getToken().pos;
         log( "illegal", pos.lineNo, pos.column );
      }
      case .Detect( codeCore ) {
         log( "outputCode" );
         codeCore.outputCode( new CodeGenerator( io.stdout ) );
         io.stdout.flush();
         log( "" );
         let token = tokenizer.getToken();
         let pos = token.pos;
         log( "%d:%d" (pos.lineNo, pos.column), token.kind.$_txt );
      }
   }
}

pub fn EbnfCtrl.setupCandidate() mut {
   let fusedCoreSet:Set<&Core> = (@);
   foreach ruleList in self.element2ruleList {
      ruleList.setupCandidate( self, fusedCoreSet );
   }
}

pub fn analyze_ebnf( tokenizer:EbnfTokenizer ) : &EbnfCtrl {

   let mut ebnfCtrl = new EbnfCtrl();
   
   while true {
      let token = tokenizer.getToken();
      if token.kind == .Eof {
         break;
      }
      if token.txt == "#" {
         tokenizer.skipLine( token );
      } elseif token.txt == "#+" {
         let nextToken = tokenizer.getToken();
         switch nextToken.txt.upper() {
            case "BEGIN_SRC" {
               tokenizer.set_inEbnf( true );
            }
            case "END_SRC" {
               tokenizer.set_inEbnf( false );
            }
         }
         tokenizer.skipLine( token );
      } elseif tokenizer.$inEbnf {
         if isElement( token ) {
            ebnfCtrl.processDecl( tokenizer, token );
         }
      }
   }

   ebnfCtrl.setupCandidate();
   
   return ebnfCtrl;
}
